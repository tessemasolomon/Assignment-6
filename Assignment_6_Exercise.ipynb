{"cells":[{"cell_type":"markdown","metadata":{"id":"gcsOB52UFcDi"},"source":["# ECE 57000 Assignment 6 Exercise\n","\n","Your Name:"]},{"cell_type":"markdown","metadata":{"id":"bYMyL9snFcDk"},"source":["Objective: Build an RNN model to predict the next character in a sequence of text data from Shakespeare's plays."]},{"cell_type":"markdown","metadata":{"id":"8BaBkDGGFcDl"},"source":["# Exercise  1: Data Preprocessing (30 points)\n","In this part, you will implement some preprocessing functions.\n","Run the following code to load the text data from the given file \"shakespeare.txt\". Do not change the random seed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"znqKOQgY5QV5"},"outputs":[],"source":["import numpy as np\n","! pip install unidecode\n","import unidecode\n","import string\n","import time\n","import torch\n","import pdb\n","\n","import torch.nn as nn\n","from torch.autograd import Variable\n","np.random.seed(123)\n","\n","all_characters = string.printable\n","print(all_characters)"]},{"cell_type":"markdown","metadata":{"id":"TmJD_PTFYR9Q"},"source":["Follow the step on the instructions and mount your google drive on Colab which allows to access the .txt file uploaded on your drive that was included with this assignment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UwUNU4ChYS89"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2NgDiYr15QV8"},"outputs":[],"source":["def read_file(filename):\n","    file = unidecode.unidecode(open(filename).read())\n","    return file\n","\n","dir_root = '/content/...'     # Your assignment dir\n","file_path = dir_root + '/shakespeare.txt'\n","file = read_file(file_path)\n","file_len = len(file)\n","print(f\"file length: {file_len}\")\n","print(file[:100])"]},{"cell_type":"markdown","metadata":{"id":"ZsQwF_CMFcDm"},"source":["### Task 1: Implement function to get a random chunk of Shakespeare text (15 points)\n","The `get_random_chunk` function is a helper function that generates a random chunk of **input text data** and **output text data** (which is one character shifted from the input) from the Shakespeare dataset.\n","Specifically, the `chunk_len` argument specifies the size of the input and output sequences.\n","For example, if `chunk_len=4`, then a valid return value would be the two chunks:\n","`('Befo','efor')` or `('proc','roce')`.\n","This function is useful\n","in generating diverse sets of input data for training the RNN model in the assignment.\n","\n","Hints:\n","- Start from a random index of the file (but note that the max index must be small enough so that a full chunk can be extracted).\n","- Based on this random start index, extract `chunk_len` characters for the input sequence and `chunk_len` characters for the output sequence (shifted one character to the right)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jlq8jmC17g40"},"outputs":[],"source":["def get_random_chunk(file, chunk_len = 100):\n","    ######### Your Code Here ###########\n","\n","    ######### End of your code #########\n","\n","curr_chunk, next_chunk = get_random_chunk(file='Hello world!', chunk_len=10)\n","print(f\"curr_chunk =>{curr_chunk}\\n next_chunk=> {next_chunk}\")\n","\n","print(f\"Is curr_chunk and next_chunk same length: {len(curr_chunk) == len(next_chunk)}\")\n","print(f\"Is next chunk shifted by one: {curr_chunk[1:] == next_chunk[:-1]}\")\n"]},{"cell_type":"markdown","metadata":{"id":"grwjvdZCFcDn"},"source":["### Task 2: Implement function to convert to tensors (15 points)\n","\n","Define a function `to_tensor(string)` that takes a string of characters as input and return torch tensor as output, similar to in the demo in class.\n","Specifically,\n","1. Create an empty tensor of shape `(len(string), 1, len(all_characters))` using the PyTorch `torch.zeros` function,\n","    where `len(string)` is the length of the input string, 1 is the batch size, and `len(all_characters)` is the total\n","    number of unique characters in the text data.\n","2. Loop through each character in the input string and convert it to a one-hot encoded vector."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p4OGaxQoFcDo"},"outputs":[],"source":["def to_tensor(string):\n","######### Your Code Here ###########\n","\n","######### End of your code #########\n","\n","def get_one_hot_tensors(input, output):\n","    return to_tensor(input), to_tensor(output)\n","\n","input, output = get_random_chunk(file, 50)\n","print(input.replace('\\n', ' '))\n","print(output.replace('\\n', ' '))\n","input_tensor, output_tensor = get_one_hot_tensors(input, output)\n","print(f\"input shape: {input_tensor.shape}\")\n","print(f\"output shape: {output_tensor.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"ollYgjInFcDo"},"source":["# Exercise  2: Build the RNN model (30 points)\n","\n","In this part, you will build the RNN model using PyTorch.\n","- nn.GRU is used to implement the GRU algorithm for processing sequential input data.\n","    - https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n","- The decoder layer is a fully connected neural network layer that maps the output of the GRU layer to the desired output size.\n","- As we are only implementing a single layer RNN, the model is not powerful enough to learn long-term dependencies in the text data. So don't be surprised if the output sentences are not very meaningful. We are providing you loss plots (`gru_loss_ex2.png`) to help you check if your code is working correctly."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Obd7_b75kiV"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","\n","class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n","        super(RNN, self).__init__()\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","\n","        # Define modules of RNN\n","        ######### Your Code Here ###########\n","        # Set `self.rnn_cell` to a nn.GRU\n","\n","        # Define a linear decoder layer that maps from the hidden size to the output size\n","\n","        ######### End of your code #########\n","\n","    def forward(self, input, hidden):\n","        ######### Your Code Here ###########\n","        # 1. Reshape the input to (1, 1, -1) and pass it to the GRU layer\n","        # 2. Reshape the rnn_cell output to (1, -1) and pass it to the decoder layer\n","\n","\n","\n","        ######### End of your code #########\n","        return output, hidden\n","\n","    def init_hidden(self):\n","        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EeF6V-lr7-iC"},"outputs":[],"source":["def train(inp, target, decoder):\n","    hidden = decoder.init_hidden()\n","    decoder.zero_grad()\n","    loss = 0\n","\n","    input_tensor, target_tensor = get_one_hot_tensors(inp, target)\n","    for c in range(len(inp)):\n","        output, hidden = decoder(input_tensor[c], hidden)\n","        loss += criterion(output, torch.argmax(target_tensor[c]).unsqueeze(0))\n","\n","    loss.backward()\n","    decoder_optimizer.step()\n","    return loss.item() / max_length"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-e9e-MMZFcDp"},"outputs":[],"source":["def evaluate(decoder, prime_str='A', predict_len=100, temperature=0.8):\n","    hidden = decoder.init_hidden()\n","    prime_input = to_tensor(prime_str)\n","    predicted = prime_str\n","\n","    # Use priming string to \"build up\" hidden state\n","    for p in range(len(prime_str) - 1):\n","        out, hidden = decoder(prime_input[p], hidden)\n","    inp = prime_input[-1]\n","    for p in range(predict_len):\n","        output, hidden = decoder(inp, hidden)\n","\n","        # Sample from the network as a multinomial distribution\n","        output_dist = output.data.view(-1).div(temperature).exp()\n","        top_i = torch.multinomial(output_dist, 1)[0]\n","\n","        # Add predicted character to string and use as next input\n","        predicted_char = all_characters[top_i]\n","        predicted += predicted_char\n","        inp = to_tensor(predicted_char)\n","\n","    return predicted"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j-rK_nLB8VyD","scrolled":false},"outputs":[],"source":["n_epochs = 2000\n","print_every = 100\n","plot_every = 10\n","hidden_size = 100\n","n_layers = 1\n","lr = 0.005\n","max_length = len(all_characters)\n","\n","decoder = RNN(max_length, hidden_size, max_length)\n","decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n","criterion = nn.CrossEntropyLoss()\n","\n","start = time.time()\n","all_losses = []\n","loss_avg = 0\n","\n","for epoch in range(1, n_epochs + 1):\n","    loss = train(*get_random_chunk(file), decoder)\n","    loss_avg += loss\n","\n","    if epoch % print_every == 0:\n","        print(f\"[({epoch} {epoch / n_epochs * 100}%) {loss}]\")\n","        print(evaluate(decoder, 'Wh', 100), '\\n')\n","\n","    if epoch % plot_every == 0:\n","        all_losses.append(loss_avg / plot_every)\n","        loss_avg = 0\n","\n","print(f\"______________________________________________________________\")\n","print(evaluate(decoder, 'Th', 200, temperature=0.2))\n","\n","import matplotlib.pyplot as plt\n","plt.plot(all_losses)\n","plt.title(\"GRU Loss: Loss vs Epoch\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Tgit73uSFcDp"},"source":["# Exercise  3: Implement an LSTM model (30 points)\n","\n","Using the equations from the slides in class, write your own LSTM cell module.\n","The code below will use this instead of the GRU cell module and train the model.\n","\n","Notes:\n","- Note that for LSTM the hidden state is really both the $h_t$ and $C_t$ so we just unpack the passed hidden state into these two variables at the beginning, and pack them into a tuple for returning.\n","- We apply a single linear layer to compute all the linear parts of the model that operate on $h'_{t-1}$ and then unpack these using `chunk(4)` into the four separate parts. This is equivalent to having 4 separate linear layers.\n","- As we are only implementing a single layer RNN, the model is not powerful enough to learn long-term dependencies in the text data. So don't be surprised if the output sentences are not very meaningful. We are providing you loss plots (`lstm_loss_ex3.png`) to help you check if your code is working correctly."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4QUw29z0FcDq"},"outputs":[],"source":["class LSTMCell(nn.Module):\n","    def __init__(self, input_size, hidden_size, bias=True):\n","        super(LSTMCell, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.bias = bias\n","\n","        self.xh = nn.Linear(input_size, hidden_size * 4, bias=bias)\n","        self.hh = nn.Linear(hidden_size, hidden_size * 4, bias=bias)\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        std = 1.0 / np.sqrt(self.hidden_size)\n","        for w in self.parameters():\n","            w.data.uniform_(-std, std)\n","\n","    def forward(self, input, hidden=None):\n","        # Unpack hidden state and cell state\n","        hx, cx = hidden\n","\n","        # Apply linear layers to input and hidden state\n","        linear = self.xh(input) + self.hh(hx)\n","\n","        # Get outputs of applying a linear transform for each part of the LSTM\n","        input_linear, forget_linear, cell_linear, output_linear = linear.reshape(-1).chunk(4)\n","\n","        ######### Your Code Here ###########\n","        # 1. Apply activation functions to get gates and new cell state information\n","        # 2. Calculate the new cell state (c_new)\n","        # 3. Calculate the new hidden state (h_new)\n","\n","\n","\n","\n","\n","\n","        ######### End of your code #########\n","\n","        # Pack cell state $C_t$ and hidden state $h_t$ into a single hidden state tuple\n","        output = h_new # For LSTM the output is just the hidden state\n","        hidden = (h_new, c_new) # Packed h and C\n","        return output, hidden\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HJMMHVt3FcDq","scrolled":false},"outputs":[],"source":["lr = 0.001\n","class LSTM_RNN(RNN):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        # Replace the gru cell with LSTM cell\n","        self.rnn_cell = LSTMCell(max_length, hidden_size, max_length)\n","\n","    def init_hidden(self):\n","        # LSTM cells need two hidden variables in a tuple of (h_t,C_t)\n","        return (Variable(torch.zeros(1, 1, self.hidden_size)), Variable(torch.zeros(1, 1, self.hidden_size)))\n","\n","decoder = LSTM_RNN(max_length, hidden_size, max_length)\n","decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n","\n","all_losses = []\n","loss_avg = 0\n","\n","for epoch in range(1, n_epochs + 1):\n","    loss = train(*get_random_chunk(file),decoder)\n","    loss_avg += loss\n","\n","    if epoch % print_every == 0:\n","        print(f\"[({epoch} {epoch / n_epochs * 100}%) {loss}]\")\n","        print(evaluate(decoder, 'Wh', 100), '\\n')\n","\n","    if epoch % plot_every == 0:\n","        all_losses.append(loss_avg / plot_every)\n","        loss_avg = 0\n","\n","print(f\"______________________________________________________________\")\n","print(evaluate(decoder, 'Th', 200, temperature=0.2))\n","\n","plt.plot(all_losses)\n","plt.title(\"LSTM Loss: Loss vs Epoch\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"JLd1t1Y4FcDq"},"source":["# Exercise 4: Implement your own GRU (10 points)\n","\n","Same as above but implement a GRU instead of an LSTM module. An exmaple of GRU architecture can be found from the lecture slide: https://www.davidinouye.com/course/ece57000-fall-2023/lectures/recurrent-neural-networks.pdf\n","\n","You output loss plot should be similar in Exercise 2."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vmeCL7AuFcDq"},"outputs":[],"source":["class GRUCell(nn.Module):\n","    def __init__(self, input_size, hidden_size, bias=True):\n","        super(GRUCell, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.bias = bias\n","\n","        self.h2z = nn.Linear(input_size + hidden_size, hidden_size)\n","        self.h2r = nn.Linear(input_size + hidden_size, hidden_size)\n","        self.h2h = nn.Linear(input_size + hidden_size, hidden_size)\n","\n","        self.reset_parameters()\n","\n","\n","    def reset_parameters(self):\n","        std = 1.0 / np.sqrt(self.hidden_size)\n","        for w in self.parameters():\n","            w.data.uniform_(-std, std)\n","\n","    def forward(self, input, hx=None):\n","        # Inputs:\n","        #       input: of shape (batch_size, input_size)\n","        #       hx: of shape (batch_size, hidden_size)\n","        # Output:\n","        #       h_t, h_t: h_t is of shape (batch_size, hidden_size)\n","\n","        if hx is None:\n","            hx = Variable(input.new_zeros(input.size(0), self.hidden_size))\n","\n","        ######### Your Code Here ###########\n","\n","        # Concatenate hidden and input to get h_prime (see torch.cat)\n","\n","        # Use self.h2z to calculate z_t\n","\n","        # Use self.h2r to calculate r_t\n","\n","        # Use Hadamard product of r_t and hx and concatenate with input\n","        # Then use h2h to calculate new hidden information h_tbar\n","\n","        # Update h_t with z_t, hx, and h_tbar\n","\n","\n","        ######### End of your code #########\n","\n","        # Reshape h_t match input size\n","        h_t = h_t.reshape(1, 1, -1)\n","\n","        return h_t, h_t   # Output and hidden are both h_t"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KreMhRIlKaWL"},"outputs":[],"source":["n_epochs = 2000\n","print_every = 100\n","plot_every = 10\n","hidden_size = 100\n","n_layers = 1\n","lr = 0.005\n","max_length = len(all_characters)\n","\n","# Replace the RNN module with your implemented GRUcell\n","class GRU_RNN(RNN):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        # Replace wtih your gru cell\n","        self.rnn_cell = GRUCell(max_length, hidden_size, max_length)\n","\n","decoder = GRU_RNN(max_length, hidden_size, max_length)\n","decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n","\n","all_losses = []\n","loss_avg = 0\n","\n","for epoch in range(1, n_epochs + 1):\n","    loss = train(*get_random_chunk(file),decoder)\n","    loss_avg += loss\n","\n","    if epoch % print_every == 0:\n","        print(f\"[({epoch} {epoch / n_epochs * 100}%) {loss}]\")\n","        print(evaluate(decoder, 'Wh', 100), '\\n')\n","\n","    if epoch % plot_every == 0:\n","        all_losses.append(loss_avg / plot_every)\n","        loss_avg = 0\n","\n","print(f\"______________________________________________________________\")\n","print(evaluate(decoder, 'Th', 200, temperature=0.2))\n","\n","plt.plot(all_losses)\n","plt.title(\"GRU Loss: Loss vs Epoch\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.show()"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}}},"nbformat":4,"nbformat_minor":0}